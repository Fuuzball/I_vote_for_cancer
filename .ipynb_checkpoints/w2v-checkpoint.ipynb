{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-773f8e70cb7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import helpers\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = helpers.train_test_split('./data/training_variants', './data/training_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will mostly follow examples from\n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "and\n",
    "https://codesachin.wordpress.com/2015/10/09/generating-a-word2vec-model-from-a-block-of-text-using-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_text = X_train.Text.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_sentences(text):\n",
    "    sentences = []\n",
    "    for document in text:\n",
    "        sts = document.split('.') # list of sentences in the document\n",
    "        for s in sts:\n",
    "            sentences.append(s.split(' '))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the w2v model on all the unique text in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = build_sentences(unique_text)\n",
    "model = gensim.models.Word2Vec(k, size=100)\n",
    "#model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.622146030897\n",
      "0.206961482879\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('tumor', 'cancer'))\n",
    "print(model.similarity('tumor', 'and'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the mean of all the word vectors in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc2vec(doc):\n",
    "    words = doc.split(' ')\n",
    "    N = 0\n",
    "    v = np.zeros(100)\n",
    "    for w in words:\n",
    "        try:\n",
    "            v+=model[w.rstrip('.')]\n",
    "            N +=1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    v = (1./N)*v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide training set into training + validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ntotal = len(X_train.Text)\n",
    "Ntr = 2124 # 80% of training set\n",
    "X_tr = X_train.Text[0:Ntr]\n",
    "X_validate = X_train.Text[Ntr:]\n",
    "y_tr = y_train.Class[0:Ntr]\n",
    "y_validate = y_train.Class[Ntr:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert every document in the training set to a vector. This takes a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "Xtr = np.zeros((Ntr, 100))\n",
    "i = 0\n",
    "for doc in X_tr:\n",
    "    Xtr[i,:] = doc2vec(doc)\n",
    "    i += 1\n",
    "    if i%500 == 0: print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction method:\n",
    "1. Get the centroid vectors from the training set.\n",
    "2. Get distance of new vector from each centroid. Probabilities are the normalized distances from each centroid.\n",
    "3. Return list of probabilities.\n",
    "\n",
    "Evaluation metric: log-loss. For some reason the sklearn log-loss gives me something different from when I compute it explicitly, so I'm probably using it wrong.\n",
    "\n",
    "Comparison to random chance where I set the probabilities to be just the underlying frequency in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss of w2v prediction\n",
      "1.78894351607\n",
      "Log-loss of random chance prediction\n",
      "2.63876002514\n",
      "Log-loss of uniform guessing\n",
      "3.16992500144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "clf = NearestCentroid()\n",
    "clf.fit(Xtr, y_tr)\n",
    "centroids = clf.centroids_\n",
    "\n",
    "def predict(X, ctr):\n",
    "    p = np.zeros((X.shape[0], 9))\n",
    "    for n in range(X.shape[0]):\n",
    "        d = np.zeros(9)\n",
    "        for i in range(9):\n",
    "            d[i] = np.linalg.norm(X[n,:] - ctr[i,:])\n",
    "        d = d/np.linalg.norm(d)\n",
    "        p[n,:] = d\n",
    "    return p\n",
    "\n",
    "def onehot(y):\n",
    "    output = np.zeros((len(y), 9))\n",
    "    for i,n in enumerate(y):\n",
    "        output[i,n-1]=1\n",
    "    return output\n",
    "\n",
    "def logloss(y, X):\n",
    "    N = X.shape[0]\n",
    "    x = 0\n",
    "    for i in range(N):\n",
    "        x+= np.dot(y[i,:], np.log2(X[i,:]))\n",
    "    x = (-1./N)*x\n",
    "    return x\n",
    "\n",
    "print('Log-loss of w2v prediction')\n",
    "print(logloss(onehot(y_tr), output))\n",
    "\n",
    "output = predict(Xtr, centroids)\n",
    "\n",
    "class_prob = np.sum(onehot(y_tr), axis=0)/len(y_tr)\n",
    "random_chance = np.zeros_like(output)\n",
    "for i in range(random_chance.shape[0]):\n",
    "    random_chance[i,:] = class_prob\n",
    "print('Log-loss of random chance prediction')\n",
    "print(logloss(onehot(y_tr), random_chance)) # random chance prediction\n",
    "\n",
    "#uniform guessing\n",
    "print('Log-loss of uniform guessing')\n",
    "print(logloss(onehot(y_tr), (1/9.)*np.ones_like(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist the w2v model to disk. It turns out to be 119M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save('firstw2vmodel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
