{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import helpers\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "en_stop = get_stop_words('en')\n",
    "p_stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2656, 5)\n",
      "(665, 5)\n"
     ]
    }
   ],
   "source": [
    "y = pd.read_csv('./data/training_variants')\n",
    "X = pd.read_csv('./data/training_text', sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\n",
    "text_train, text_test, variants_train, variants_test = train_test_split(X, y, test_size=0.2, \\\n",
    "                                                                        random_state=0, stratify=None)\n",
    "\n",
    "train_full = variants_train.merge(text_train, how='inner', on='ID')\n",
    "test_full = variants_test.merge(text_test, how='inner', on='ID')\n",
    "\n",
    "print(train_full.shape)\n",
    "print(test_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mini_doc = train_full['Text'][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in mini_doc:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328.32379603385925\n"
     ]
    }
   ],
   "source": [
    "n_topics = 10\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "t0 = time.time()\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=n_topics, id2word = dictionary, passes=20)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.027*\"mutat\" + 0.012*\"cell\" + 0.009*\"pten\" + 0.008*\"tumor\" + 0.008*\"activ\" + 0.007*\"gene\" + 0.007*\"2\"')\n",
      "(1, '0.027*\"mutat\" + 0.022*\"egfr\" + 0.020*\"patient\" + 0.012*\"1\" + 0.011*\"exon\" + 0.011*\"0\" + 0.009*\"2\"')\n",
      "(2, '0.018*\"cell\" + 0.017*\"p53\" + 0.012*\"mutat\" + 0.010*\"mutant\" + 0.008*\"et\" + 0.008*\"al\" + 0.008*\"express\"')\n",
      "(3, '0.019*\"cell\" + 0.012*\"mutat\" + 0.008*\"1\" + 0.007*\"tumor\" + 0.007*\"patient\" + 0.007*\"gene\" + 0.007*\"activ\"')\n",
      "(4, '0.017*\"ra\" + 0.014*\"cell\" + 0.011*\"activ\" + 0.011*\"1\" + 0.009*\"ar\" + 0.008*\"protein\" + 0.007*\"express\"')\n",
      "(5, '0.021*\"p53\" + 0.012*\"mutat\" + 0.009*\"protein\" + 0.008*\"cell\" + 0.008*\"function\" + 0.008*\"figur\" + 0.007*\"mutant\"')\n",
      "(6, '0.028*\"met\" + 0.024*\"cell\" + 0.015*\"c\" + 0.010*\"receptor\" + 0.010*\"mutat\" + 0.010*\"express\" + 0.009*\"tumor\"')\n",
      "(7, '0.016*\"1\" + 0.014*\"mutat\" + 0.009*\"variant\" + 0.008*\"cell\" + 0.008*\"2\" + 0.008*\"tumor\" + 0.007*\"gene\"')\n",
      "(8, '0.018*\"mutat\" + 0.017*\"cell\" + 0.014*\"activ\" + 0.009*\"flt3\" + 0.009*\"kinas\" + 0.009*\"mutant\" + 0.008*\"resist\"')\n",
      "(9, '0.026*\"brca1\" + 0.023*\"variant\" + 0.012*\"assay\" + 0.012*\"function\" + 0.010*\"mutat\" + 0.009*\"protein\" + 0.008*\"activ\"')\n"
     ]
    }
   ],
   "source": [
    "for line in (ldamodel.print_topics(num_topics=n_topics, num_words=7)):\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
