{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import helpers\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "en_stop = get_stop_words('en')\n",
    "p_stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_txt, train_var = helpers.get_1st_layer_data('./data/training_variants', './data/training_text')\n",
    "val_txt, val_var = helpers.get_2nd_layer_data('./data/training_variants', './data/training_text')\n",
    "test_txt, test_var = helpers.get_test_for_final_score('./data/training_variants', './data/training_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Word2Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_corpus(text_df, tagged=True):\n",
    "    for i, t in enumerate(text_df):\n",
    "        bow = gensim.utils.simple_preprocess(t)\n",
    "        if tagged:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(\n",
    "                bow, [i]\n",
    "            )\n",
    "        else:\n",
    "            yield bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 s, sys: 160 ms, total: 21.2 s\n",
      "Wall time: 21.2 s\n",
      "CPU times: user 2.94 s, sys: 24 ms, total: 2.97 s\n",
      "Wall time: 2.97 s\n",
      "CPU times: user 6.33 s, sys: 48 ms, total: 6.38 s\n",
      "Wall time: 6.37 s\n"
     ]
    }
   ],
   "source": [
    "%time train_corpus = list(make_corpus(train_txt['Text']))\n",
    "%time val_corpus = list(make_corpus(val_txt['Text'], tagged=False))\n",
    "%time test_corpus = list(make_corpus(test_txt['Text'], tagged=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d2v_size = 100\n",
    "def run_model(d2v_size):\n",
    "    model = gensim.models.doc2vec.Doc2Vec(size=d2v_size, min_count=0, iter=5)\n",
    "    %time model.build_vocab(train_corpus)\n",
    "    %time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "    doc_vects = []\n",
    "    for a in model.docvecs:\n",
    "        doc_vects.append(a)\n",
    "    doc_arr = np.array(doc_vects)\n",
    "    clf = OneVsRestClassifier(svm.SVC(C=1.0, kernel='linear', probability=True,\n",
    "                                     random_state=0))\n",
    "    y_train_bi = label_binarize(train_var[\"Class\"], classes=range(1, 10))\n",
    "\n",
    "    %time clf.fit(doc_arr, y_train_bi)\n",
    "\n",
    "    test_arr = np.zeros((len(val_corpus), d2v_size))\n",
    "    for i, t in enumerate(val_corpus):\n",
    "        if i % 20 == 0:\n",
    "            print(\"\\r\", i, end=\"\")\n",
    "        test_arr[i] = model.infer_vector(t)\n",
    "\n",
    "\n",
    "    y_test_prob = clf.predict_proba(test_arr)\n",
    "    y_test = val_var[\"Class\"]\n",
    "    logLoss = log_loss(y_test, y_test_prob, eps=1e-15, normalize=True, labels=range(1, 10))\n",
    "    return logLoss, doc_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.8 s, sys: 0 ns, total: 3.8 s\n",
      "Wall time: 3.79 s\n",
      "CPU times: user 1min 11s, sys: 208 ms, total: 1min 11s\n",
      "Wall time: 27.2 s\n",
      "CPU times: user 1.31 s, sys: 0 ns, total: 1.31 s\n",
      "Wall time: 1.3 s\n",
      " 320CPU times: user 3.72 s, sys: 0 ns, total: 3.72 s\n",
      "Wall time: 3.72 s\n",
      "CPU times: user 1min 33s, sys: 164 ms, total: 1min 33s\n",
      "Wall time: 33.5 s\n",
      "CPU times: user 3.78 s, sys: 0 ns, total: 3.78 s\n",
      "Wall time: 3.77 s\n",
      " 320CPU times: user 3.51 s, sys: 0 ns, total: 3.51 s\n",
      "Wall time: 3.51 s\n",
      "CPU times: user 2min 3s, sys: 196 ms, total: 2min 4s\n",
      "Wall time: 42.9 s\n",
      "CPU times: user 6.05 s, sys: 0 ns, total: 6.05 s\n",
      "Wall time: 6.05 s\n",
      " 320CPU times: user 3.72 s, sys: 0 ns, total: 3.72 s\n",
      "Wall time: 3.72 s\n",
      "CPU times: user 2min 40s, sys: 284 ms, total: 2min 41s\n",
      "Wall time: 54.8 s\n",
      "CPU times: user 9.83 s, sys: 0 ns, total: 9.83 s\n",
      "Wall time: 9.83 s\n",
      " 320CPU times: user 3.76 s, sys: 8 ms, total: 3.76 s\n",
      "Wall time: 3.76 s\n",
      "CPU times: user 4min 8s, sys: 188 ms, total: 4min 8s\n",
      "Wall time: 1min 23s\n",
      "CPU times: user 21.2 s, sys: 0 ns, total: 21.2 s\n",
      "Wall time: 21.2 s\n",
      " 320CPU times: user 3.59 s, sys: 0 ns, total: 3.59 s\n",
      "Wall time: 3.59 s\n",
      "CPU times: user 5min 30s, sys: 252 ms, total: 5min 30s\n",
      "Wall time: 1min 50s\n",
      "CPU times: user 34 s, sys: 0 ns, total: 34 s\n",
      "Wall time: 34 s\n",
      " 320CPU times: user 3.84 s, sys: 0 ns, total: 3.84 s\n",
      "Wall time: 3.85 s\n",
      "CPU times: user 6min 53s, sys: 204 ms, total: 6min 53s\n",
      "Wall time: 2min 18s\n",
      "CPU times: user 38.3 s, sys: 0 ns, total: 38.3 s\n",
      "Wall time: 38.3 s\n",
      " 320CPU times: user 3.87 s, sys: 4 ms, total: 3.88 s\n",
      "Wall time: 3.88 s\n",
      "CPU times: user 8min 14s, sys: 208 ms, total: 8min 14s\n",
      "Wall time: 2min 44s\n",
      "CPU times: user 38.8 s, sys: 4 ms, total: 38.8 s\n",
      "Wall time: 38.8 s\n",
      " 320CPU times: user 3.67 s, sys: 4 ms, total: 3.67 s\n",
      "Wall time: 3.67 s\n",
      "CPU times: user 9min 33s, sys: 252 ms, total: 9min 33s\n",
      "Wall time: 3min 11s\n",
      "CPU times: user 39.5 s, sys: 4 ms, total: 39.5 s\n",
      "Wall time: 39.5 s\n",
      " 320"
     ]
    }
   ],
   "source": [
    "size_list = [8, 20, 32, 52, 100, 152, 200, 252, 300]\n",
    "loss_list = []\n",
    "for size in size_list:\n",
    "    loss_list.append(run_model(size)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.87 s, sys: 8 ms, total: 3.88 s\n",
      "Wall time: 3.88 s\n",
      "CPU times: user 5min 31s, sys: 248 ms, total: 5min 31s\n",
      "Wall time: 1min 50s\n",
      "CPU times: user 31.9 s, sys: 0 ns, total: 31.9 s\n",
      "Wall time: 31.9 s\n",
      " 320"
     ]
    }
   ],
   "source": [
    "train_arr_d2v = run_model(152)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 660"
     ]
    }
   ],
   "source": [
    "test_arr = np.zeros((len(test_corpus), d2v_size))\n",
    "for i, t in enumerate(test_corpus):\n",
    "    if i % 20 == 0:\n",
    "        print(\"\\r\", i, end=\"\")\n",
    "    test_arr[i] = model.infer_vector(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arr_d2v = test_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    tf-dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.8 s, sys: 20 ms, total: 24.8 s\n",
      "Wall time: 24.8 s\n",
      "CPU times: user 11.5 s, sys: 0 ns, total: 11.5 s\n",
      "Wall time: 11.5 s\n",
      "CPU times: user 1min, sys: 0 ns, total: 1min\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
       "  verbose=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_size = 300\n",
    "\n",
    "\n",
    "##Pipeline\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=1, max_features=16000, strip_accents='unicode',lowercase =True,\n",
    "    analyzer='word', use_idf=True, \n",
    "    smooth_idf=True, sublinear_tf=True, stop_words = 'english')\n",
    "ffilter = SelectKBest(mutual_info_classif, k=tf_idf_size)\n",
    "clf = OneVsRestClassifier(svm.SVC(C=1.0, kernel='linear', probability=True,\n",
    "                                 random_state=0))\n",
    "##Data and labels\n",
    "X_train = train_txt[\"Text\"]\n",
    "X_test = val_txt[\"Text\"]\n",
    "y_train = train_var[\"Class\"]\n",
    "y_test = val_var[\"Class\"]\n",
    "y_train_bi = label_binarize(y_train, classes=range(1, 10))\n",
    "y_test_bi = label_binarize(y_test, classes=range(1, 10))\n",
    "\n",
    "##Fitting\n",
    "%time tfidf.fit(X_train)\n",
    "Xtr_train = tfidf.transform(X_train)\n",
    "\n",
    "%time ffilter.fit(Xtr_train, y_train)\n",
    "\n",
    "%time clf.fit(ffilter.transform(Xtr_train), y_train_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3833228403932973"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate\n",
    "Xtr_test = tfidf.transform(X_test)\n",
    "y_test_prob = clf.predict_proba(ffilter.transform(Xtr_test))\n",
    "log_loss(y_test, y_test_prob, eps=1e-15, normalize=True, labels=range(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_arr_tfidf = ffilter.transform(Xtr_train).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<665x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 156666 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test_txt[\"Text\"]\n",
    "Xtr_test = tfidf.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arr_tfidf = ffilter.transform(Xtr_test).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(doc):\n",
    "    # clean and tokenize document string\n",
    "    raw = doc.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    #stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n"
     ]
    }
   ],
   "source": [
    "# list for tokenized documents in loop\n",
    "train_corpus = []\n",
    "\n",
    "# loop through document list\n",
    "for idx, doc in enumerate(train_txt['Text']):\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)\n",
    "    train_corpus.append(get_tokens(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 58s, sys: 1h 17s, total: 1h 30min 16s\n",
      "Wall time: 14min 39s\n"
     ]
    }
   ],
   "source": [
    "n_topics = 50\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(train_corpus)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in train_corpus]\n",
    "\n",
    "# generate LDA model\n",
    "%time ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=n_topics, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n"
     ]
    }
   ],
   "source": [
    "n_train = len(train_corpus)\n",
    "train_lda = np.zeros((n_train, n_topics))\n",
    "for i in range(n_train):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    bow = corpus[i]\n",
    "    for idx, prob in ldamodel[bow]:\n",
    "        train_lda[i, idx] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "n_test = len(test_corpus)\n",
    "test_lda = np.zeros((n_test, n_topics))\n",
    "\n",
    "test_doc = test_txt['Text']\n",
    "for i, doc in enumerate(test_doc):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    tokens = get_tokens(doc)\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    for idx, prob in ldamodel[bow]:\n",
    "        test_lda[i, idx] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(\n",
    "    svm.SVC(C=1., kernel='linear', probability=True, random_state=42)\n",
    ")\n",
    "\n",
    "y_train = label_binarize(\n",
    "    train_var['Class'], classes = range(1, 10)\n",
    ")\n",
    "y_test = label_binarize(\n",
    "    test_var['Class'], classes = range(1, 10)\n",
    ")\n",
    "\n",
    "clf.fit(train_lda, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5226731038981935"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prob = clf.predict_proba(test_lda)\n",
    "log_loss(y_test, test_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_arr_lda = train_lda\n",
    "test_arr_lda = test_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2324, 152)\n",
      "(2324, 50)\n",
      "(2324, 300)\n",
      "(665, 100)\n",
      "(665, 50)\n",
      "(665, 300)\n"
     ]
    }
   ],
   "source": [
    "print(train_arr_d2v.shape)\n",
    "print(train_arr_lda.shape)\n",
    "print(train_arr_tfidf.shape)\n",
    "\n",
    "print(test_arr_d2v.shape)\n",
    "print(test_arr_lda.shape)\n",
    "print(test_arr_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_train_arr = np.hstack((train_arr_d2v, train_arr_lda, train_arr_tfidf))\n",
    "combined_test_arr = np.hstack((test_arr_d2v, test_arr_lda, test_arr_tfidf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 450)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_arr.shape\n",
    "combined_test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
